import time
import pandas as pd
import os
from tqdm.notebook import tqdm
import re

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_experimental_option(
        "prefs", {
            # block image loading
            "profile.managed_default_content_settings.images": 2,
        }
    )



def enlist_restaurants(url):  
    # Initialize the WebDriver (e.g., Chrome)
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    try:
        # Open the webpage
        driver.get(url)

        # Accept cookies if the popup is present
        try:
            cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), "Accetta") or contains(text(), "accetto")]')))
            cookie_button.click()
            time.sleep(1)  # Adjust this time if necessary
            print("Cookie button clicked.")
        except Exception as e:
            print("No cookie pop-up detected or unable to close it:", e)

        # Handle the popup after accepting cookies
        try:
            popup_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, 'ccl-388f3fb1d79d6a36.ccl-6d2d597727bd7bab.ccl-59eced23a4d9e077.ccl-7be8185d0a980278')))
            popup_button.click()
            time.sleep(1)  # Adjust this time if necessary
            print("Popup button clicked.")
        except Exception as e:
            print("No popup detected or unable to close it:", e)

        # Wait for the elements to load after clicking the button
        time.sleep(3)  # Adjust the sleep time as necessary or use more sophisticated waits

        # Get the page source after the elements have loaded
        page_source = driver.page_source

        # Parse the page source with BeautifulSoup
        soup = BeautifulSoup(page_source, 'html.parser')

        # Find all list elements with the specified class
        list_items = soup.find_all('li', class_='HomeFeedGrid-b0432362335be7af')

        # Extract href and aria-label from the desired elements within each list item
        result = []
        for item in list_items:
            a_element = item.find('a', class_='HomeFeedUICard-3e299003014c14f9')
            if a_element:
                href = "https://deliveroo.it" + a_element.get('href')
                aria_label = a_element.get('aria-label')
                result.append({'href': href, 'aria-label': aria_label})

        # Create a DataFrame from the result list
        df = pd.DataFrame(result)


    finally:
        # Close the WebDriver
        driver.quit()

        return df

def collect_info_on_restaurants(df):

    df.loc[:, 'text'] = 0
   #df = df.iloc[0:10, :]
    for res in tqdm(df['href']):
        # Initialize WebDriver (this will automatically download the correct version of ChromeDriver)
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

        # Define the URL to test
        url = res  # Replace with the actual URL from your DataFrame

        # Open the URL
        driver.get(url)

        # Wait for the page to load completely
        time.sleep(1.5)  # Adjust this time as necessary

        try:
            # Handle the cookie pop-up if it appears
            # Accept cookies if the popup is present
            try:
                cookie_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), "Accetta") or contains(text(), "accetto")]')))
                cookie_button.click()
                time.sleep(0.5)  # Adjust this time if necessary
                print("Cookie button clicked.")
            except Exception as e:
                print("No cookie pop-up detected or unable to close it:", e)

            
            #Handle "Restaurant can't accept orders"
            time.sleep(0.5)
            try:
                close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, "ccl-388f3fb1d79d6a36.ccl-9ed29b91bb2d9d02.ccl-59eced23a4d9e077.ccl-7be8185d0a980278")))
                close_button.click()
                time.sleep(1)  # Adjust this time if necessary
                print("Close button clicked.")
            except Exception as e:
                print("Close button for 'Restaurant can't accept orders' pop-up not found:", e)
        

                
            # Locate the parent element that contains the button
            parent_element = driver.find_element(By.CLASS_NAME, "UIHeaderInfoRows-023104015d7355b8.ccl-a5e1512b87ef2079")
            
            # Find and click the button within this parent element
            button = parent_element.find_element(By.CLASS_NAME, "ccl-4704108cacc54616.ccl-4f99b5950ce94015.ccl-724e464f2f033893")
            button.click()
            
            # Wait for the pop-up to load
            time.sleep(0.5)  # Adjust this time as necessary
            
            # Find the parent element of the popup
            popup_element = driver.find_element(By.CLASS_NAME, "ccl-9237ff225b0d2789.ccl-e770a464a394226e.ccl-bae50209245d2304")
        
            # Find all elements with the class name "UIContentCard-32d54d142ca96f5c" within the popup
            link_elements = popup_element.find_elements(By.CLASS_NAME, "UIContentCard-32d54d142ca96f5c")
            
            # Extract the href attribute from the second link element
            if len(link_elements) >= 2:
                link = link_elements[1].get_attribute("href")
                df.loc[df['href']==res, 'Link'] = link
                
                # Find all elements with the class name "UILines-eb427a2507db75b3" within the popup
                text_elements = popup_element.find_elements(By.CLASS_NAME, "UILines-eb427a2507db75b3")
                
                # Extract the text from the second text element
                tex = ''
                for t in text_elements:
                    tex = tex + '^' + t.text
                df.loc[df['href']==res, 'text'] = tex
            else:
                print("Second link element not found")

        except Exception as e:
            print("Error:", e)

        # Close the WebDriver
        driver.quit()
    
    return df

def main():

    url = "https://deliveroo.it/it/restaurants/rimini/rimini/?geohash=srbfwh60jm2f&collection=top+rated"
    df = enlist_restaurants(url)
    df = collect_info_on_restaurants(df)

    return df


df = main()


df['text'] = df['text'].map(lambda x: str(x).split('^')[1:])

def extract_phone_number(text_list):
    phone_pattern = re.compile(r'\+?\d[\d -]{8,12}\d')
    for text in text_list:
        match = phone_pattern.search(text)
        if match:
            return match.group()
    return None


def extract_address(text_list):
    address_pattern = re.compile(r'.*[a-zA-Z].*\d{5}$')
    for text in text_list:
        if address_pattern.match(text) and not text.startswith("Chiama"):
            return text
    return None

df['phone_number'] = df['text'].apply(extract_phone_number)
df['address'] = df['text'].apply(extract_address)

# Function to handle the aggregation logic
def aggregate_column(series):
    if series.nunique() == 1:
        return series.iloc[0]
    else:
        return series.unique().tolist()

# Select only the columns needed for aggregation
columns_to_aggregate = ['href', 'aria-label', 'Link', 'phone_number']

# Group by "address" and aggregate
grouped = df.groupby('address')[columns_to_aggregate].agg({
    'href': 'count',  # Count the number of occurrences for "n°"
    'aria-label': aggregate_column,
    'Link': aggregate_column,
    'phone_number': aggregate_column
}).reset_index()

# Add the 'text' column with a custom aggregation
grouped['text'] = df.groupby('address')['text'].apply(lambda x: ', '.join(map(str, x))).values

# Rename the columns as needed
grouped.rename(columns={'href': 'n°'}, inplace=True)
# Sort the DataFrame by the "n°" column
grouped = grouped.sort_values(by='n°', ascending=False)
#grouped



def extract_values(text):
    n_ratings = re.findall(r'(\d+)\s+recensioni', text)
    rating_medio = re.findall(r'Valutato\s+come\s+(\d+\.\d+)', text)
    if not n_ratings:
        n_ratings = ['500']
    return n_ratings, rating_medio

def extract_values_from_list(texts):
    n_ratings = []
    rating_medio = []
    for text in texts:
        n, r = extract_values(text)
        n_ratings.extend(n)
        rating_medio.extend(r)
    return n_ratings, rating_medio

grouped[['n° ratings', 'Rating medio']] = grouped['aria-label'].apply(lambda x: pd.Series(extract_values_from_list(x) if isinstance(x, list) else extract_values(x)))

grouped.to_excel("cinisello.xlsx")
